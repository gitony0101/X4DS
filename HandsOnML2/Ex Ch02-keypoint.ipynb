{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2 keypoint EX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With increasing demand in machine learning and data science in businesses, for **upgraded data strategizing** there’s a need for a **better workflow** to **ensure robustness in data modelling**. \n",
    "\n",
    "Machine learning has certain steps to be followed namely – data collection, data preprocessing(cleaning and feature engineering), model training, validation and prediction on the test data(which is previously unseen by model). \n",
    "\n",
    "Here testing data needs to go through the same preprocessing as training data.**For this iterative process**, **pipelines** are used which can automate the entire process for both training and testing data. It ensures reusability of the model by reducing the redundant part, thereby speeding up the process. This could prove to be very effective during the production workflow.\n",
    "\n",
    "**Advantages of using Pipeline:**\n",
    "\n",
    "- Automating the workflow being iterative.\n",
    "- Easier to fix bugs \n",
    "- Production Ready\n",
    "- Clean code writing standards\n",
    "- Helpful in iterative hyperparameter tuning and cross-validation evaluation\n",
    "\n",
    "**Challenges in using Pipeline:**\n",
    "\n",
    "- Proper data cleaning\n",
    "- Data Exploration and Analysis\n",
    "- Efficient feature engineering\n",
    "\n",
    "**Scikit-Learn Pipeline**\n",
    "\n",
    "The sklearn.pipeline module implements utilities to build a composite estimator, as a chain of transforms and estimators.\n",
    "\n",
    "With the **scikit learn pipeline**, we can easily systemise the process and therefore make it extremely reproducible.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline 1 : [Daily bike share](https://towardsdatascience.com/step-by-step-tutorial-of-sci-kit-learn-pipeline-62402d5629b6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "instant         int64\n",
       "dteday         object\n",
       "season          int64\n",
       "yr              int64\n",
       "mnth            int64\n",
       "holiday         int64\n",
       "weekday         int64\n",
       "workingday      int64\n",
       "weathersit      int64\n",
       "temp          float64\n",
       "atemp         float64\n",
       "hum           float64\n",
       "windspeed     float64\n",
       "rentals         int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"./data/daily-bike-share.csv\")\n",
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "instant       0\n",
       "dteday        0\n",
       "season        0\n",
       "yr            0\n",
       "mnth          0\n",
       "holiday       0\n",
       "weekday       0\n",
       "workingday    0\n",
       "weathersit    0\n",
       "temp          0\n",
       "atemp         0\n",
       "hum           0\n",
       "windspeed     0\n",
       "rentals       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()  # clear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label\n",
    "data = data[\n",
    "    [\n",
    "        \"season\",\n",
    "        \"mnth\",\n",
    "        \"holiday\",\n",
    "        \"weekday\",\n",
    "        \"workingday\",\n",
    "        \"weathersit\",\n",
    "        \"temp\",\n",
    "        \"atemp\",\n",
    "        \"hum\",\n",
    "        \"windspeed\",\n",
    "        \"rentals\",\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = data.drop(\"rentals\", axis=1)\n",
    "y = data[\"rentals\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=123\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the pipeline\n",
    "\n",
    "The main parameter of a pipeline we’ll be working on is ‘steps’.\n",
    "\n",
    " From the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html#examples-using-sklearn-pipeline-pipeline),it is a *‘list of (name, transform) tuples (implementing fit/transform) that are chained, in the order in which they are chained, with the last object an estimator.'*\n",
    "\n",
    "It’s easier to just have a glance at what the pipeline should look like:\n",
    "\n",
    "        Pipeline(steps=[('name_of_preprocessor', preprocessor),\n",
    "                        ('name_of_ml_model', ml_model())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OrdinalEncoder, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_transformer = Pipeline(\n",
    "    steps=[(\"imputer\", SimpleImputer(strategy=\"mean\")), (\"scaler\", StandardScaler())]\n",
    ")\n",
    "\n",
    "categorical_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"constant\")),\n",
    "        (\"encoder\", OrdinalEncoder()),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Apply ColumnTransformer\n",
    "\n",
    "Specify which columns are numeric and which are categorical, so we can apply the transformers accordingly.\n",
    "\n",
    " We apply the transformers to features by using `ColumnTransformer`. \n",
    " \n",
    " Applying the transformers to features is our preprocessor. \n",
    " \n",
    " Similar to pipeline, we pass **a list of tuples**, which is composed of *(‘name’, ‘transformer’, ‘features’)*, to the parameter **‘transformers’.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = [\"temp\", \"atemp\", \"hum\", \"windspeed\"]\n",
    "\n",
    "categorical_features = [\n",
    "    \"season\",\n",
    "    \"mnth\",\n",
    "    \"holiday\",\n",
    "    \"weekday\",\n",
    "    \"workingday\",\n",
    "    \"weathersit\",\n",
    "]\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\n",
    "            \"numeric\",\n",
    "            numeric_transformer,\n",
    "            numeric_features,\n",
    "        ),  ## Specify, or \"map\" the num pipeline with the num features\n",
    "        (\n",
    "            \"categorical\",\n",
    "            categorical_transformer,\n",
    "            categorical_features,\n",
    "        ),  ## Specify, or \"map\" the cat pipeline with the cat features\n",
    "    ]\n",
    ")\n",
    "## Not preferred:\n",
    "# numeric_features = data.select_dtypes(include=['int64', 'float64']).columns\n",
    "# categorical_features = data.select_dtypes(include=['object']).drop(['Loan_Status'], axis=1).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimator with Random Forest Regression model\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\"preprocessor\", preprocessor),  # preprocessor from the ColumnTransformer\n",
    "        (\"regressor\", RandomForestRegressor()),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we make antoher **pipeline** with previos preprocessor,which made of two **pipelines**:\n",
    "\n",
    "`numeric_transformer & category_transformer`,meanwhile they all come from basic pipelines.\n",
    "\n",
    "Also, RandomForestRegressor added up in the new pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('preprocessor',\n",
      "                 ColumnTransformer(transformers=[('numeric',\n",
      "                                                  Pipeline(steps=[('imputer',\n",
      "                                                                   SimpleImputer()),\n",
      "                                                                  ('scaler',\n",
      "                                                                   StandardScaler())]),\n",
      "                                                  ['temp', 'atemp', 'hum',\n",
      "                                                   'windspeed']),\n",
      "                                                 ('categorical',\n",
      "                                                  Pipeline(steps=[('imputer',\n",
      "                                                                   SimpleImputer(strategy='constant')),\n",
      "                                                                  ('encoder',\n",
      "                                                                   OrdinalEncoder())]),\n",
      "                                                  ['season', 'mnth', 'holiday',\n",
      "                                                   'weekday', 'workingday',\n",
      "                                                   'weathersit'])])),\n",
      "                ('regressor', RandomForestRegressor())])\n"
     ]
    }
   ],
   "source": [
    "# Train the model:\n",
    "rf_model = pipeline.fit(X_train, y_train)\n",
    "print(rf_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazing and beautiful, isn't it?\n",
    "\n",
    "It makes clear of pipeline working process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7621930012568542\n"
     ]
    }
   ],
   "source": [
    "predictions = rf_model.predict(X_test)\n",
    "r2_pred = r2_score(y_test, predictions)\n",
    "print(r2_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the model\n",
    "\n",
    "To maximise reproducibility, we‘d like to use this model repeatedly for our new incoming data. Let’s save the model by using ‘joblib’ package to save it as a pickle file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./rf_model.pkl']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(rf_model, \"./rf_model.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can call this pipeline, which includes all sorts of data preprocessing we need and the training model, whenever we need it：\n",
    "\n",
    "        # In other notebooks \n",
    "        rf_model = joblib.load('PATH/TO/rf_model.pkl')\n",
    "        \n",
    "        new_prediction = rf_model.predict(new_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "Scikit learn pipeline makes workflows smoother and more flexible. \n",
    "\n",
    "For example, you can easily compare the performance of a number of algorithms like:\n",
    "\n",
    "        regressors = [\n",
    "            regressor_1()\n",
    "           ,regressor_2()\n",
    "           ,regressor_3()\n",
    "           ....]for regressor in regressors:\n",
    "            pipeline = Pipeline(steps = [\n",
    "                       ('preprocessor', preprocessor)\n",
    "                      ,('regressor',regressor)\n",
    "                   ])\n",
    "            model = pipeline.fit(X_train, y_train)\n",
    "            predictions = model.predict(X_test)\n",
    "            print (regressor)\n",
    "            print (f('Model r2 score:{r2_score(predictions, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or adjust the preprocessing/transforming methods. For instance, use ‘median’ value to fill missing values, use a different scaler for numeric features, change to one-hot encoding instead of ordinal encoding to handle categorical features, hyperparameter tuning, etc.\n",
    "\n",
    "        numeric_transformer = Pipeline(steps=[\n",
    "               ('imputer', SimpleImputer(strategy='median'))\n",
    "              ,('scaler', MinMaxScaler())\n",
    "        ])categorical_transformer = Pipeline(steps=[\n",
    "               ('imputer', SimpleImputer(strategy='constant'))\n",
    "              ,('encoder', OneHotEncoder())\n",
    "        ])pipeline = Pipeline(steps = [\n",
    "                       ('preprocessor', preprocessor)\n",
    "                      ,('regressor',RandomForestRegressor(n_estimators=300\n",
    "                                                         ,max_depth=10))\n",
    "                   ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline 2: [Iris dataset](https://analyticsindiamag.com/hands-on-tutorial-on-machine-learning-pipelines-with-scikit-learn/) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iris_df.data\n",
    "# iris_df.feature_names\n",
    "# iris_df.target.shape\n",
    "# iris_df.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    iris_df.data, iris_df.target, test_size=0.3, random_state=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_lr = Pipeline(\n",
    "    [  # \"steps=\"\" omitted.\n",
    "        (\"scalar_1\", StandardScaler()),\n",
    "        (\"pca_1\", PCA(n_components=2)),\n",
    "        (\"lr_classifier\", LogisticRegression(random_state=0)),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8666666666666667"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = pipeline_lr.fit(X_train, y_train)\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Move on:Stacking Multiple Pipelines to Find the Model with the Best Accuracy\n",
    "\n",
    "*With Iris dataset*\n",
    "\n",
    "We build different pipelines for each algorithm and the fit to see which performs better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_lr = Pipeline(\n",
    "    [\n",
    "        (\"scalar_1\", StandardScaler()),\n",
    "        (\"pca_1\", PCA(n_components=2)),\n",
    "        (\"lr_classifier\", LogisticRegression(random_state=0)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipeline_dt = Pipeline(\n",
    "    [\n",
    "        (\"scalar_2\", StandardScaler()),\n",
    "        (\"pca_2\", PCA(n_components=2)),\n",
    "        (\"dt_classifier\", DecisionTreeClassifier()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipeline_svm = Pipeline(\n",
    "    [(\"scalar_3\", StandardScaler()), (\"pca_3\", PCA(n_components=2)), (\"clf\", svm.SVC())]\n",
    ")\n",
    "\n",
    "pipeline_knn = Pipeline(\n",
    "    [\n",
    "        (\"scalar_4\", StandardScaler()),\n",
    "        (\"pca_4\", PCA(n_components=2)),\n",
    "        (\"knn_classifier\", KNeighborsClassifier()),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression  Test Accuracy: 0.8666666666666667\n",
      "Decision Tree  Test Accuracy: 0.9111111111111111\n",
      "Support Vector Machine  Test Accuracy: 0.9333333333333333\n",
      "K Nearest Neighbor  Test Accuracy: 0.9111111111111111\n"
     ]
    }
   ],
   "source": [
    "pipelines = [pipeline_lr, pipeline_dt, pipeline_svm, pipeline_knn]\n",
    "\n",
    "pipeline_dict = {\n",
    "    0: \"Logistic Regression\",\n",
    "    1: \"Decision Tree\",\n",
    "    2: \"Support Vector Machine\",\n",
    "    3: \"K Nearest Neighbor\",\n",
    "}\n",
    "for pipe in pipelines:\n",
    "    pipe.fit(X_train, y_train)\n",
    "\n",
    "for i, model in enumerate(pipelines):\n",
    "    print(f\"{pipeline_dict[i]}  Test Accuracy: {model.score(X_test,y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Go deeply:Hyperparameter Tuning in Pipeline\n",
    "\n",
    "With pipelines, you can easily perform a grid-search over a set of parameters for each step of this meta-estimator to find the best performing parameters. \n",
    "\n",
    "To do this you first need to create a parameter grid for your chosen model. \n",
    "\n",
    "One important thing to note is that you need to append the name that you have given the classifier part of your pipeline to each parameter name. In my code above I have called this ‘randomforestclassifier’ so I have added randomforestclassifier__ to each parameter. \n",
    "\n",
    "Next, I created a grid search object which includes the original pipeline. When I then call fit, the transformations are applied to the data, before a cross-validated grid-search is performed over the parameter grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = make_pipeline((RandomForestClassifier()))\n",
    "grid_param = [\n",
    "    {\n",
    "        \"randomforestclassifier\": [RandomForestClassifier()],\n",
    "        \"randomforestclassifier__n_estimators\": [10, 100, 1000],\n",
    "        \"randomforestclassifier__max_depth\": [5, 8, 15, 25, 30, None],\n",
    "        \"randomforestclassifier__min_samples_leaf\": [1, 2, 5, 10, 15, 100],\n",
    "        \"randomforestclassifier__max_leaf_nodes\": [2, 5, 10],\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch = GridSearchCV(pipe, grid_param, cv=5, verbose=0, n_jobs=-1)\n",
    "best_model = gridsearch.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9777777777777777"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('randomforestclassifier',\n",
       "                 RandomForestClassifier(max_depth=5, max_leaf_nodes=10,\n",
       "                                        min_samples_leaf=5))])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'randomforestclassifier': RandomForestClassifier(max_depth=5, max_leaf_nodes=10, min_samples_leaf=5),\n",
       " 'randomforestclassifier__max_depth': 5,\n",
       " 'randomforestclassifier__max_leaf_nodes': 10,\n",
       " 'randomforestclassifier__min_samples_leaf': 5,\n",
       " 'randomforestclassifier__n_estimators': 100}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9619047619047618"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('randomforestclassifier',\n",
       "                                        RandomForestClassifier())]),\n",
       "             n_jobs=-1,\n",
       "             param_grid=[{'randomforestclassifier': [RandomForestClassifier(max_depth=5,\n",
       "                                                                            max_leaf_nodes=10,\n",
       "                                                                            min_samples_leaf=5)],\n",
       "                          'randomforestclassifier__max_depth': [5, 8, 15, 25,\n",
       "                                                                30, None],\n",
       "                          'randomforestclassifier__max_leaf_nodes': [2, 5, 10],\n",
       "                          'randomforestclassifier__min_samples_leaf': [1, 2, 5,\n",
       "                                                                       10, 15,\n",
       "                                                                       100],\n",
       "                          'randomforestclassifier__n_estimators': [10, 100,\n",
       "                                                                   1000]}])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "This is a basic pipeline implementation. In real-life data science, scenario data would need to be prepared first then applied pipeline for rest processes. \n",
    "\n",
    "Building quick and efficient machine learning models is what pipelines are for. Pipelines are high in demand as it helps in coding better and extensible in implementing big data projects. \n",
    "\n",
    "Automating the applied machine learning workflow and saving time invested in redundant preprocessing work."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "daabc12b7e82262630a7e373b09e73b659ba21f5700e5baa669a48eb8260a031"
  },
  "kernelspec": {
   "display_name": "p39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
